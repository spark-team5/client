import { useEffect, useRef, useState } from "react";
import { detectEmotion } from "@/shared/api/faceDetection";  
import { useNavigate } from "react-router-dom"; 

export const useFaceCapture = () => {
  const navigate = useNavigate();
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const [loading, setLoading] = useState(false);
  const [message, setMessage] = useState("ì›ì— ì–¼êµ´ì„ ë§ì¶°ì£¼ì„¸ìš”.");

  useEffect(() => {
    startCamera();
    return () => stopCamera();
  }, []);

  const startCamera = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: "user", width: { ideal: 1920 }, height: { ideal: 1080 } },
        audio: false,
      });
      if (videoRef.current) {
        videoRef.current.srcObject = stream;
      }
    } catch (err) {
      console.error("ğŸš¨ ì›¹ìº  ì ‘ê·¼ ì˜¤ë¥˜:", err);
      alert("ì¹´ë©”ë¼ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì € ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.");
    }
  };

  const stopCamera = () => {
    if (videoRef.current && videoRef.current.srcObject) {
      const stream = videoRef.current.srcObject as MediaStream;
      stream.getTracks().forEach((track) => track.stop());
    }
  };

  const captureAndAnalyze = async () => {
    if (!videoRef.current || !canvasRef.current) return;

    setLoading(true);
    setMessage("ë¶„ì„ ì¤‘...");

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;
      const ctx = canvas.getContext("2d");

      if (ctx) {
        canvas.width = video.videoWidth;
        canvas.height = video.videoHeight;
        ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
      }

      video.style.display = "none";
      canvas.style.display = "block";

      const imageBase64 = canvas.toDataURL("image/png").split(",")[1];
      
      try {
        const result = await detectEmotion(imageBase64);
        console.log("âœ… ê°ì • ë¶„ì„ ê²°ê³¼:", result);

        if (result && result.statusMessage) {
          setMessage(result.statusMessage);
        }

        setTimeout(() => navigate("/face-result"), 2000);
      } catch (error) {
        console.error("ì„œë²„ ìš”ì²­ ì‹¤íŒ¨:", error);
        setMessage("ì„œë²„ ì˜¤ë¥˜ ë°œìƒ");
      }
    } finally {
      setLoading(false);
    }
  };

  return {
    videoRef,
    canvasRef,
    loading,
    message,
    captureAndAnalyze,
  };
};